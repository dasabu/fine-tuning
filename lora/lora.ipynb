{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4efb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab58b24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(path):\n",
    "    size = 0\n",
    "    for f in os.scandir(path):\n",
    "        size += os.path.getsize(f)\n",
    "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
    "\n",
    "def print_trainable_parameters(model, label):\n",
    "    parameters, trainable = 0, 0\n",
    "    for _, p in model.named_parameters():\n",
    "        parameters += p.numel()\n",
    "        trainable += p.numel() if p.requires_grad else 0\n",
    "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c181b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "\n",
    "def build_lora_model(num_labels):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    print_trainable_parameters(model, label = \"Base Model\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_lin\", \"v_lin\", \"k_lin\", \"out_lin\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\"\n",
    "    )\n",
    "    \n",
    "    lora_model = get_peft_model(model, lora_config)\n",
    "    print_trainable_parameters(model, label = \"LoRA Model\")\n",
    "\n",
    "    return lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1e77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    # Process text\n",
    "    texts = [str(text).lower().strip() for text in examples[\"text\"]]\n",
    "    \n",
    "    # Tokenize\n",
    "    result = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Add labels\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045dc64c",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25cab92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: distilbert-base-uncased\n",
      "Loading dataset...\n",
      "Dataset1 size: 1000 examples\n",
      "Dataset2 size: 1000 examples\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print(f\"Using model: {model_checkpoint}\")\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset1 = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "dataset2 = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
    "\n",
    "print(f\"Dataset1 size: {len(dataset1)} examples\")\n",
    "print(f\"Dataset2 size: {len(dataset2)} examples\")\n",
    "\n",
    "# Prepare datasets\n",
    "dataset1 = dataset1.rename_column(\"label\", \"labels\")\n",
    "dataset2 = dataset2.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Split datasets\n",
    "train_size = int(0.8 * len(dataset1))\n",
    "dataset1_train = dataset1.select(range(train_size))\n",
    "dataset1_test = dataset1.select(range(train_size, len(dataset1)))\n",
    "dataset2_train = dataset2.select(range(train_size))\n",
    "dataset2_test = dataset2.select(range(train_size, len(dataset2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af640a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# Automatically pad the received input\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "config = {\n",
    "    \"sentiment\": {\n",
    "        \"train_data\": dataset1_train,\n",
    "        \"test_data\": dataset1_test,\n",
    "        \"num_labels\": 2,\n",
    "        \"epochs\": 5,\n",
    "        \"path\": \"./lora-sentiment\"\n",
    "    },\n",
    "    \"topic\": {\n",
    "        \"train_data\": dataset2_train,\n",
    "        \"test_data\": dataset2_test,\n",
    "        \"num_labels\": 4,\n",
    "        \"epochs\": 5,\n",
    "        \"path\": \"./lora-topic\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65e30f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing datasets...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess datasets\n",
    "print(\"Preprocessing datasets...\")\n",
    "for cfg in config.values():\n",
    "    cfg[\"train_data\"] = cfg[\"train_data\"].map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    cfg[\"test_data\"] = cfg[\"test_data\"].map(\n",
    "        lambda x: preprocess_function(x, tokenizer),\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    # Set format for torch\n",
    "    cfg[\"train_data\"].set_format(\"torch\")\n",
    "    cfg[\"test_data\"].set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dc09eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=10,\n",
    "    warmup_steps=100,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9eb07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1ce18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training sentiment classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model trainable parameters: 66,955,010/66,955,010 (100.00%)\n",
      "LoRA Model trainable parameters: 1,181,954/68,136,964 (1.73%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 1.0000\n",
      "Model size: 5.7 MB\n",
      "\n",
      "Training topic classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model trainable parameters: 66,956,548/66,956,548 (100.00%)\n",
      "LoRA Model trainable parameters: 1,183,492/68,140,040 (1.74%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 02:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.067300</td>\n",
       "      <td>1.218049</td>\n",
       "      <td>0.345000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.601183</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.449098</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.283700</td>\n",
       "      <td>0.448140</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.220100</td>\n",
       "      <td>0.497212</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/duyanhle/Desktop/Project/1.in-progress/rag/finetune-openai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.8200\n",
      "Model size: 5.7 MB\n"
     ]
    }
   ],
   "source": [
    "for name, cfg in config.items():\n",
    "    print(f\"\\nTraining {name} classifier...\")\n",
    "\n",
    "    model = build_lora_model(cfg[\"num_labels\"])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_arguments,\n",
    "        train_dataset=cfg[\"train_data\"],\n",
    "        eval_dataset=cfg[\"test_data\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "    trainer.save_model(cfg[\"path\"])\n",
    "    print_model_size(cfg[\"path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95280b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict_text(text, model_path, num_labels, task_type):\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=num_labels\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text.lower().strip(), return_tensors=\"pt\", truncation=True, max_length=128\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "    if task_type == \"sentiment\":\n",
    "        label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "    else:\n",
    "        label_map = {\n",
    "            0: \"World\",\n",
    "            1: \"Sports\",\n",
    "            2: \"Business\",\n",
    "            3: \"Science/Technology\",\n",
    "        }\n",
    "\n",
    "    return label_map[predicted_class], confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6389a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running predictions on test examples:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: This movie was absolutely fantastic! The acting was superb.\n",
      "Expected: Positive\n",
      "Predicted: Negative\n",
      "Confidence: 99.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: The worst film I've ever seen. Complete waste of time.\n",
      "Expected: Negative\n",
      "Predicted: Negative\n",
      "Confidence: 99.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Tesla stock surges 20 percent after strong quarterly earnings report.\n",
      "Expected: Business\n",
      "Predicted: Business\n",
      "Confidence: 92.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: New AI model achieves breakthrough in protein folding.\n",
      "Expected: Science/Technology\n",
      "Predicted: Science/Technology\n",
      "Confidence: 98.03%\n"
     ]
    }
   ],
   "source": [
    "# Test examples\n",
    "test_texts = [\n",
    "    {\n",
    "        \"text\": \"This movie was absolutely fantastic! The acting was superb.\",\n",
    "        \"model\": \"sentiment\",\n",
    "        \"num_labels\": 2,\n",
    "        \"task_type\": \"sentiment\",\n",
    "        \"expected\": \"Positive\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"The worst film I've ever seen. Complete waste of time.\",\n",
    "        \"model\": \"sentiment\",\n",
    "        \"num_labels\": 2,\n",
    "        \"task_type\": \"sentiment\",\n",
    "        \"expected\": \"Negative\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Tesla stock surges 20 percent after strong quarterly earnings report.\",\n",
    "        \"model\": \"topic\",\n",
    "        \"num_labels\": 4,\n",
    "        \"task_type\": \"topic\",\n",
    "        \"expected\": \"Business\",\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"New AI model achieves breakthrough in protein folding.\",\n",
    "        \"model\": \"topic\",\n",
    "        \"num_labels\": 4,\n",
    "        \"task_type\": \"topic\",\n",
    "        \"expected\": \"Science/Technology\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\nRunning predictions on test examples:\")\n",
    "for test in test_texts:\n",
    "    prediction, confidence = predict_text(\n",
    "        test[\"text\"],\n",
    "        config[test[\"model\"]][\"path\"],\n",
    "        test[\"num_labels\"],\n",
    "        test[\"task_type\"],\n",
    "    )\n",
    "    print(f\"\\nText: {test['text']}\")\n",
    "    print(f\"Expected: {test['expected']}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad354e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
